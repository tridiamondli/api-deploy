#!/usr/bin/env python3
"""
æ—¥å¿—ç®¡ç†å·¥å…·
ç”¨äºæŸ¥çœ‹ã€åˆ†æå’Œç®¡ç†APIè¯·æ±‚æ—¥å¿—
"""

import os
import json
import argparse
import sys
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Iterator, Optional
import glob

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
    
    def get_log_files(self) -> List[Path]:
        """è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ï¼ˆä»…æ”¯æŒæœªå‹ç¼©çš„.logæ–‡ä»¶ï¼‰"""
        if not self.log_dir.exists():
            return []
        
        # åªæŸ¥æ‰¾æ™®é€šçš„.logæ–‡ä»¶
        log_files = []
        log_files.extend(self.log_dir.glob("*.log"))
        
        return sorted(log_files)
    
    def _open_log_file(self, file_path: Path):
        """æ‰“å¼€æ—¥å¿—æ–‡ä»¶ï¼ˆä»…æ”¯æŒæ™®é€šæ–‡æœ¬æ–‡ä»¶ï¼‰"""
        return open(file_path, 'r', encoding='utf-8')
    
    def _parse_timestamp(self, timestamp_str: str) -> Optional[datetime]:
        """å¥å£®çš„æ—¶é—´æˆ³è§£æ"""
        # ç§»é™¤å¯èƒ½çš„å¾®ç§’éƒ¨åˆ†
        clean_timestamp = timestamp_str.split('.')[0]
        
        # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
        formats = [
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%dT%H:%M:%S",
            "%Y-%m-%d %H:%M",
            "%m-%d %H:%M:%S",
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(clean_timestamp, fmt)
            except ValueError:
                continue
                
        return None
    
    def parse_log_line(self, line: str) -> Dict:
        """æ”¹è¿›çš„æ—¥å¿—è¡Œè§£æ"""
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›´å‡†ç¡®åœ°è§£ææ—¥å¿—æ ¼å¼
            # æ ¼å¼: YYYY-MM-DD HH:mm:ss | LEVEL | location | message
            pattern = r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(?:\.\d+)?)\s*\|\s*([A-Z]+)\s*\|\s*([^|]+)\s*-\s*(.+)$'
            match = re.match(pattern, line.strip())
            
            if match:
                timestamp, level, location, message = match.groups()
                
                # å°è¯•è§£æç»“æ„åŒ–æ¶ˆæ¯
                if " | " in message:
                    msg_parts = message.split(" | ", 1)
                    event_type = msg_parts[0].strip()
                    
                    if len(msg_parts) > 1:
                        json_part = msg_parts[1].strip()
                        try:
                            json_data = json.loads(json_part)
                            return {
                                "timestamp": timestamp,
                                "level": level.strip(),
                                "location": location.strip(),
                                "event_type": event_type,
                                "data": json_data,
                                "parsed": True
                            }
                        except json.JSONDecodeError:
                            # JSONè§£æå¤±è´¥ï¼Œå½“ä½œæ™®é€šæ¶ˆæ¯å¤„ç†
                            pass
                
                return {
                    "timestamp": timestamp,
                    "level": level.strip(),
                    "location": location.strip(),
                    "message": message.strip(),
                    "parsed": True
                }
            
            # å¦‚æœæ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç®€å•çš„åˆ†å‰²
            parts = line.split(" | ", 3)
            if len(parts) >= 4:
                return {
                    "timestamp": parts[0].strip(),
                    "level": parts[1].strip(),
                    "location": parts[2].strip(),
                    "message": parts[3].strip(),
                    "parsed": True
                }
                
        except Exception as e:
            # è§£æå¤±è´¥æ—¶è®°å½•åŸå§‹è¡Œ
            return {
                "raw_line": line.strip(),
                "parse_error": str(e),
                "parsed": False
            }
        
        return {
            "raw_line": line.strip(),
            "parsed": False
        }
    
    def analyze_requests(self, days: int = 1) -> Dict:
        """åˆ†æè¯·æ±‚ç»Ÿè®¡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        cutoff_time = datetime.now() - timedelta(days=days)
        
        stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "endpoints": {},
            "error_types": {},
            "auth_failures": 0,
            "client_ips": {}
        }
        
        print(f"ğŸ“Š å¼€å§‹åˆ†ææœ€è¿‘ {days} å¤©çš„æ—¥å¿—...")
        log_files = self.get_log_files()
        
        for i, log_file in enumerate(log_files):
            print(f"å¤„ç†æ–‡ä»¶ {i+1}/{len(log_files)}: {log_file.name}")
            self._process_log_file(log_file, cutoff_time, stats)
        
        print("\nâœ… ç»Ÿè®¡åˆ†æå®Œæˆ")
        return stats
        
    def _process_log_file(self, log_file: Path, cutoff_time: datetime, stats: Dict):
        """å¤„ç†å•ä¸ªæ—¥å¿—æ–‡ä»¶çš„ç»Ÿè®¡"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œè·³è¿‡å¤ªå¤§çš„æ–‡ä»¶
            file_size = log_file.stat().st_size
            if file_size > 100 * 1024 * 1024:  # 100MB
                print(f"âš ï¸  è·³è¿‡å¤§æ–‡ä»¶: {log_file.name} ({file_size / 1024 / 1024:.1f}MB)", file=sys.stderr)
                return
            
            with self._open_log_file(log_file) as f:
                line_count = 0
                for line in f:
                    line_count += 1
                    if line_count % 10000 == 0:  # æ¯å¤„ç†10000è¡Œæ˜¾ç¤ºè¿›åº¦
                        print(f"å¤„ç† {log_file.name}: {line_count} è¡Œ", end='\r')
                    
                    parsed = self.parse_log_line(line)
                    
                    if not parsed.get("parsed") or "data" not in parsed:
                        continue
                    
                    data = parsed["data"]
                    event_type = parsed.get("event_type", "")
                    
                    # æ£€æŸ¥æ—¶é—´æ˜¯å¦åœ¨èŒƒå›´å†…
                    log_time = self._parse_timestamp(parsed["timestamp"])
                    if not log_time or log_time < cutoff_time:
                        continue
                    
                    # ç»Ÿè®¡é€»è¾‘
                    if "REQUEST_END" in event_type:
                        stats["total_requests"] += 1
                        
                        if data.get("success", False):
                            stats["successful_requests"] += 1
                        else:
                            stats["failed_requests"] += 1
                            error = data.get("error", "Unknown")
                            stats["error_types"][error] = stats["error_types"].get(error, 0) + 1
                        
                        # ç»Ÿè®¡ç«¯ç‚¹
                        endpoint = data.get("endpoint", "unknown")
                        if endpoint not in stats["endpoints"]:
                            stats["endpoints"][endpoint] = {"count": 0, "success": 0, "failed": 0}
                        
                        stats["endpoints"][endpoint]["count"] += 1
                        if data.get("success", False):
                            stats["endpoints"][endpoint]["success"] += 1
                        else:
                            stats["endpoints"][endpoint]["failed"] += 1
                    
                    elif "REQUEST_START" in event_type:
                        client_ip = data.get("client_ip", "unknown")
                        stats["client_ips"][client_ip] = stats["client_ips"].get(client_ip, 0) + 1
                    
                    elif "AUTH_FAILURE" in event_type:
                        stats["auth_failures"] += 1
                
        except Exception as e:
            print(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}", file=sys.stderr)
        
        return stats
    
    def show_recent_errors(self, hours: int = 24, limit: int = 10) -> Iterator[Dict]:
        """æ˜¾ç¤ºæœ€è¿‘çš„é”™è¯¯ï¼ˆç”Ÿæˆå™¨ç‰ˆæœ¬ï¼Œå†…å­˜å‹å¥½ï¼‰"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        error_count = 0
        
        # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´å€’åºå¤„ç†ï¼ˆæœ€æ–°çš„æ–‡ä»¶å…ˆå¤„ç†ï¼‰
        log_files = sorted(self.get_log_files(), 
                          key=lambda x: x.stat().st_mtime, 
                          reverse=True)
        
        for log_file in log_files:
            # è·³è¿‡å¤ªæ—§çš„æ–‡ä»¶
            file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
            if file_time < cutoff_time - timedelta(days=1):
                continue
            
            try:
                with self._open_log_file(log_file) as f:
                    # è¯»å–æ‰€æœ‰è¡Œå¹¶å€’åºå¤„ç†ï¼ˆæœ€æ–°çš„è¡Œå…ˆå¤„ç†ï¼‰
                    all_lines = f.readlines()
                    
                    for line in reversed(all_lines):
                        parsed = self.parse_log_line(line)
                        
                        if not parsed.get("parsed"):
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯æˆ–è­¦å‘Š
                        is_error = (parsed.get("level") in ["ERROR", "WARNING"] or 
                                   "ERROR" in parsed.get("event_type", "") or
                                   "AUTH_FAILURE" in parsed.get("event_type", ""))
                        
                        if is_error:
                            log_time = self._parse_timestamp(parsed["timestamp"])
                            if log_time and log_time >= cutoff_time:
                                yield parsed
                                error_count += 1
                                
                                if error_count >= limit:
                                    return
                    
            except Exception as e:
                print(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}", file=sys.stderr)
    
    def cleanup_old_logs(self, days: int = 30):
        """æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶"""
        cutoff_time = datetime.now() - timedelta(days=days)
        cleaned_files = []
        
        for log_file in self.get_log_files():
            try:
                file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
                if file_time < cutoff_time:
                    log_file.unlink()
                    cleaned_files.append(log_file.name)
            except Exception as e:
                print(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}", file=sys.stderr)
        
        return cleaned_files

def main():
    parser = argparse.ArgumentParser(description="APIæ—¥å¿—ç®¡ç†å·¥å…·")
    parser.add_argument("--log-dir", default="logs", help="æ—¥å¿—ç›®å½•è·¯å¾„")
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # åˆ—å‡ºæ—¥å¿—æ–‡ä»¶
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶")
    
    # åˆ†æè¯·æ±‚ç»Ÿè®¡
    stats_parser = subparsers.add_parser("stats", help="åˆ†æè¯·æ±‚ç»Ÿè®¡")
    stats_parser.add_argument("--days", type=int, default=1, help="åˆ†ææœ€è¿‘å‡ å¤©çš„æ•°æ®")
    
    # æ˜¾ç¤ºæœ€è¿‘é”™è¯¯
    errors_parser = subparsers.add_parser("errors", help="æ˜¾ç¤ºæœ€è¿‘çš„é”™è¯¯")
    errors_parser.add_argument("--hours", type=int, default=24, help="æ˜¾ç¤ºæœ€è¿‘å‡ å°æ—¶çš„é”™è¯¯")
    errors_parser.add_argument("--limit", type=int, default=10, help="æ˜¾ç¤ºé”™è¯¯çš„æœ€å¤§æ•°é‡")
    
    # æ¸…ç†æ—§æ—¥å¿—
    cleanup_parser = subparsers.add_parser("cleanup", help="æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶")
    cleanup_parser.add_argument("--days", type=int, default=30, help="æ¸…ç†å¤šå°‘å¤©å‰çš„æ—¥å¿—")
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer(args.log_dir)
    
    if args.command == "list":
        files = analyzer.get_log_files()
        if files:
            print("ğŸ“ æ—¥å¿—æ–‡ä»¶åˆ—è¡¨:")
            for file in files:
                stat = file.stat()
                size = stat.st_size
                mtime = datetime.fromtimestamp(stat.st_mtime)
                print(f"  {file.name} ({size:,} bytes, {mtime.strftime('%Y-%m-%d %H:%M:%S')})")
        else:
            print("ğŸ“ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
    
    elif args.command == "stats":
        stats = analyzer.analyze_requests(args.days)
        
        print(f"ğŸ“Š æœ€è¿‘ {args.days} å¤©çš„è¯·æ±‚ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"  æˆåŠŸè¯·æ±‚: {stats['successful_requests']}")
        print(f"  å¤±è´¥è¯·æ±‚: {stats['failed_requests']}")
        print(f"  è®¤è¯å¤±è´¥: {stats['auth_failures']}")
        
        if stats['endpoints']:
            print(f"\nğŸ“ ç«¯ç‚¹ç»Ÿè®¡:")
            for endpoint, data in sorted(stats['endpoints'].items(), key=lambda x: x[1]['count'], reverse=True):
                print(f"  {endpoint}: {data['count']} æ¬¡ (æˆåŠŸ: {data['success']}, å¤±è´¥: {data['failed']})")
        
        if stats['client_ips']:
            print(f"\nğŸŒ å®¢æˆ·ç«¯IPç»Ÿè®¡:")
            for ip, count in sorted(stats['client_ips'].items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {ip}: {count} æ¬¡")
        
        if stats['error_types']:
            print(f"\nâŒ é”™è¯¯ç±»å‹ç»Ÿè®¡:")
            for error, count in sorted(stats['error_types'].items(), key=lambda x: x[1], reverse=True):
                print(f"  {error}: {count} æ¬¡")
    
    elif args.command == "errors":
        print(f"âŒ æŸ¥æ‰¾æœ€è¿‘ {args.hours} å°æ—¶çš„é”™è¯¯ (æœ€å¤š {args.limit} æ¡)...")
        
        error_found = False
        for error in analyzer.show_recent_errors(args.hours, args.limit):
            if not error_found:
                print(f"\nâŒ æœ€è¿‘ {args.hours} å°æ—¶çš„é”™è¯¯:")
                error_found = True
                
            timestamp = error.get("timestamp", "unknown")
            level = error.get("level", "unknown")
            message = error.get("message", "")
            data = error.get("data", {})
            
            print(f"\nâ° {timestamp} [{level}]")
            if data:
                print(f"  ğŸ“ ç«¯ç‚¹: {data.get('endpoint', 'unknown')}")
                print(f"  ğŸ’¬ é”™è¯¯: {data.get('error', data.get('message', 'unknown'))}")
                if 'client_ip' in data:
                    print(f"  ğŸŒ å®¢æˆ·ç«¯: {data['client_ip']}")
            else:
                print(f"  ğŸ’¬ æ¶ˆæ¯: {message}")
        
        if not error_found:
            print(f"âœ… æœ€è¿‘ {args.hours} å°æ—¶å†…æ²¡æœ‰é”™è¯¯")
    
    elif args.command == "cleanup":
        cleaned = analyzer.cleanup_old_logs(args.days)
        
        if cleaned:
            print(f"ğŸ—‘ï¸  æ¸…ç†äº† {len(cleaned)} ä¸ªæ—§æ—¥å¿—æ–‡ä»¶:")
            for file in cleaned:
                print(f"  - {file}")
        else:
            print(f"âœ… æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§æ—¥å¿—æ–‡ä»¶ (è¶…è¿‡ {args.days} å¤©)")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
